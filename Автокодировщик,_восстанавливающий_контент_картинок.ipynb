{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJsHj7yktISi"
      },
      "outputs": [],
      "source": [
        "Ultra PRO задание. Вариант 2\n",
        "\n",
        "Возьмите любую базу картинок (Mnist, Fashion Mnist, Cifar10 или собственную)\n",
        "На картинках в случайных местах сделайте квадраты разного размера. Варианты\n",
        "размера квадратов - на ваш выбор\n",
        "\n",
        "Обучите автокодировщик восстанавливать контент картинок, который скрыт\n",
        "квадратами\n",
        "*работать может не идеально\n",
        "**это не совсем автокодировщик, так как у автокодировщика вход строго равен выходу, но\n",
        "архитектура очень близкая к автокодировщику"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "s3qMweQTsyzc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, Conv2DTranspose, concatenate, Activation, MaxPooling2D, Conv2D, BatchNormalization, UpSampling2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cFza4x4UEHH",
        "outputId": "6e07972a-f7e4-42d1-f6f5-a78b48cd28f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "(xTrainMnist, yTrainMnist), (xTestMnist, yTestMnist) = mnist.load_data()\n",
        "\n",
        "xTrainMnist = xTrainMnist.astype('float32')/255\n",
        "\n",
        "xTrainMnist = xTrainMnist.reshape(-1, 28, 28, 1)\n",
        "\n",
        "xTrainMnist.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(xTrainMnist[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "u2_O4bDkOKuD",
        "outputId": "855d568c-3f00-422f-b32d-91ecc05a669a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdf5ed0ba30>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(xTrainMnist, yTrainMnist), (xTestMnist, yTestMnist) = mnist.load_data()\n",
        "xTrainMnist_new = xTrainMnist.copy()\n",
        "for i in range(len(xTrainMnist_new)):\n",
        "  x_size, y_size = 8, 8\n",
        "  x_rnd, y_rnd = random.randint(0, 28-x_size), random.randint(0, 28-y_size)\n",
        "  for x in range(x_rnd, x_rnd + x_size):\n",
        "    for y in range(y_rnd, y_rnd + y_size):\n",
        "        xTrainMnist_new[i][x][y] = 128 # вариант [x, y] = 128 тоже работает\n",
        "print(len(xTrainMnist_new))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH0ZI3HIL6Qa",
        "outputId": "e8329e4d-6c25-4c87-b24a-9abdf66283ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(xTrainMnist_new[11])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "d8qr_1onx2CZ",
        "outputId": "d629bf29-33f1-424a-ec51-963a41b8e525"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdf5c51cac0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(xTrainMnist, yTrainMnist), (xTestMnist, yTestMnist) = mnist.load_data()\n",
        "xTestMnist_new = xTestMnist.copy()\n",
        "for i in range(len(xTestMnist_new)):\n",
        "  x_size, y_size = 8, 8\n",
        "  x_rnd, y_rnd = random.randint(0, 28-x_size), random.randint(0, 28-y_size)\n",
        "  for x in range(x_rnd, x_rnd + x_size):\n",
        "    for y in range(y_rnd, y_rnd + y_size):\n",
        "        xTestMnist_new[i][x][y] = 128 # вариант [x, y] = 128 тоже работает\n",
        "print(len(xTestMnist_new))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XW_gsHjwfNh",
        "outputId": "01527a34-9cd3-4f6b-e7d0-f6b8b9054ba8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "P2cBxx_zUGBz",
        "outputId": "00a81c40-a76e-4168-d1e8-658f99d7f0fa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.imshow(xTestMnist[11], cmap='gray', interpolation='none');\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(xTestMnist_new[11], cmap='gray', interpolation='none');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Автокодировщик"
      ],
      "metadata": {
        "id": "tONHLMWdztTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gf0lZaTFY2Bx"
      },
      "outputs": [],
      "source": [
        "def baseAutoencoder(shape=(28,28,1)):\n",
        "    img_input = Input((shape))\n",
        "\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(img_input)\n",
        "    x = BatchNormalization()(x) # затем пропускаем через слой нормализации данных\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x) # далее снова слой двумерной свёртки\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D()(x) # передаём на слой подвыборки, снижающий размерность поступивших на него данных\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    z = MaxPooling2D()(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', activation='relu')(z) # слой разжимает данные(с 28*20 на 56*40)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "\n",
        "    x = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', activation='relu')(x) # слой разжимает данные(с 56*40 на 112*80)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = Model(img_input, x)\n",
        "    model.compile(optimizer=Adam(lr=0.0001),\n",
        "             loss='mean_squared_error')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelAutoMnist = baseAutoencoder((28,28,1))\n",
        "modelAutoMnist.fit(xTrainMnist_new[:2000], xTrainMnist[:2000], epochs=512, batch_size=128, validation_data = (xTestMnist_new[-500:], xTestMnist[-500:]))\n",
        "\n",
        "\n",
        "modelAutoMnist.save_weights('modelAutoMnist.h5')\n",
        "modelAutoMnist.load_weights('modelAutoMnist.h5')\n",
        "modelAutoMnist.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTo74Vxpzs3i",
        "outputId": "222de5a7-ef06-43f6-a9ef-de798dcd164e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/512\n",
            "16/16 [==============================] - 31s 2s/step - loss: 7196.5737 - val_loss: 6968.6299\n",
            "Epoch 2/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.3979 - val_loss: 6968.5068\n",
            "Epoch 3/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.3721 - val_loss: 6968.6602\n",
            "Epoch 4/512\n",
            "16/16 [==============================] - 30s 2s/step - loss: 7192.3608 - val_loss: 6969.6709\n",
            "Epoch 5/512\n",
            "16/16 [==============================] - 28s 2s/step - loss: 7192.3525 - val_loss: 6971.2979\n",
            "Epoch 6/512\n",
            "16/16 [==============================] - 29s 2s/step - loss: 7192.3462 - val_loss: 6972.9365\n",
            "Epoch 7/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.3389 - val_loss: 6973.5518\n",
            "Epoch 8/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.3330 - val_loss: 6973.6172\n",
            "Epoch 9/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.3271 - val_loss: 6972.8711\n",
            "Epoch 10/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.3213 - val_loss: 6971.9321\n",
            "Epoch 11/512\n",
            "16/16 [==============================] - 30s 2s/step - loss: 7192.3159 - val_loss: 6970.9844\n",
            "Epoch 12/512\n",
            "16/16 [==============================] - 32s 2s/step - loss: 7192.3115 - val_loss: 6970.2979\n",
            "Epoch 13/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.3071 - val_loss: 6969.7622\n",
            "Epoch 14/512\n",
            "16/16 [==============================] - 28s 2s/step - loss: 7192.3018 - val_loss: 6969.4307\n",
            "Epoch 15/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2974 - val_loss: 6969.1699\n",
            "Epoch 16/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2939 - val_loss: 6968.8516\n",
            "Epoch 17/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2900 - val_loss: 6968.6909\n",
            "Epoch 18/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2861 - val_loss: 6968.5815\n",
            "Epoch 19/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2832 - val_loss: 6968.4971\n",
            "Epoch 20/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2803 - val_loss: 6968.4292\n",
            "Epoch 21/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2773 - val_loss: 6968.3892\n",
            "Epoch 22/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2749 - val_loss: 6968.3911\n",
            "Epoch 23/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2725 - val_loss: 6968.3511\n",
            "Epoch 24/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2695 - val_loss: 6968.3330\n",
            "Epoch 25/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2681 - val_loss: 6968.2891\n",
            "Epoch 26/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2646 - val_loss: 6968.3208\n",
            "Epoch 27/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2627 - val_loss: 6968.3047\n",
            "Epoch 28/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2598 - val_loss: 6968.2969\n",
            "Epoch 29/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2588 - val_loss: 6968.2642\n",
            "Epoch 30/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2598 - val_loss: 6968.2739\n",
            "Epoch 31/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2554 - val_loss: 6968.2559\n",
            "Epoch 32/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2529 - val_loss: 6968.2622\n",
            "Epoch 33/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2510 - val_loss: 6968.2544\n",
            "Epoch 34/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2495 - val_loss: 6968.2471\n",
            "Epoch 35/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2471 - val_loss: 6968.2939\n",
            "Epoch 36/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2466 - val_loss: 6968.2461\n",
            "Epoch 37/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2500 - val_loss: 6968.2739\n",
            "Epoch 38/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2451 - val_loss: 6968.2588\n",
            "Epoch 39/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2432 - val_loss: 6968.2598\n",
            "Epoch 40/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2412 - val_loss: 6968.2930\n",
            "Epoch 41/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2437 - val_loss: 6968.2671\n",
            "Epoch 42/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2432 - val_loss: 6968.2769\n",
            "Epoch 43/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2402 - val_loss: 6968.2778\n",
            "Epoch 44/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2378 - val_loss: 6968.2930\n",
            "Epoch 45/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2363 - val_loss: 6968.3164\n",
            "Epoch 46/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2383 - val_loss: 6968.2388\n",
            "Epoch 47/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2378 - val_loss: 6968.2710\n",
            "Epoch 48/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2363 - val_loss: 6968.2393\n",
            "Epoch 49/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2373 - val_loss: 6968.3291\n",
            "Epoch 50/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2324 - val_loss: 6968.3027\n",
            "Epoch 51/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2319 - val_loss: 6968.2944\n",
            "Epoch 52/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2314 - val_loss: 6968.3232\n",
            "Epoch 53/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2285 - val_loss: 6968.4297\n",
            "Epoch 54/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2363 - val_loss: 6968.2100\n",
            "Epoch 55/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2422 - val_loss: 6968.2944\n",
            "Epoch 56/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2339 - val_loss: 6968.2432\n",
            "Epoch 57/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2305 - val_loss: 6968.2710\n",
            "Epoch 58/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2290 - val_loss: 6968.2378\n",
            "Epoch 59/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2246 - val_loss: 6968.3003\n",
            "Epoch 60/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2251 - val_loss: 6968.2344\n",
            "Epoch 61/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2280 - val_loss: 6968.2983\n",
            "Epoch 62/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2261 - val_loss: 6968.2944\n",
            "Epoch 63/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2222 - val_loss: 6968.2695\n",
            "Epoch 64/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2290 - val_loss: 6968.3096\n",
            "Epoch 65/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2271 - val_loss: 6968.2690\n",
            "Epoch 66/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2222 - val_loss: 6968.2544\n",
            "Epoch 67/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2207 - val_loss: 6968.2720\n",
            "Epoch 68/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2197 - val_loss: 6968.6997\n",
            "Epoch 69/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2231 - val_loss: 6968.2734\n",
            "Epoch 70/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2241 - val_loss: 6968.4443\n",
            "Epoch 71/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2246 - val_loss: 6968.2544\n",
            "Epoch 72/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2222 - val_loss: 6968.2573\n",
            "Epoch 73/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2280 - val_loss: 6968.2578\n",
            "Epoch 74/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2280 - val_loss: 6968.2598\n",
            "Epoch 75/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2231 - val_loss: 6968.2520\n",
            "Epoch 76/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2310 - val_loss: 6968.2515\n",
            "Epoch 77/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2271 - val_loss: 6968.2378\n",
            "Epoch 78/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2222 - val_loss: 6968.2407\n",
            "Epoch 79/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2183 - val_loss: 6968.2778\n",
            "Epoch 80/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2168 - val_loss: 6968.3174\n",
            "Epoch 81/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2207 - val_loss: 6968.2427\n",
            "Epoch 82/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2251 - val_loss: 6968.2451\n",
            "Epoch 83/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2261 - val_loss: 6968.2363\n",
            "Epoch 84/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2271 - val_loss: 6968.2139\n",
            "Epoch 85/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2231 - val_loss: 6968.2471\n",
            "Epoch 86/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2168 - val_loss: 6968.3081\n",
            "Epoch 87/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2148 - val_loss: 6968.3862\n",
            "Epoch 88/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2134 - val_loss: 6968.4268\n",
            "Epoch 89/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2148 - val_loss: 6968.4858\n",
            "Epoch 90/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2217 - val_loss: 6968.3350\n",
            "Epoch 91/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2148 - val_loss: 6968.2964\n",
            "Epoch 92/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2251 - val_loss: 6968.2168\n",
            "Epoch 93/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2305 - val_loss: 6968.2373\n",
            "Epoch 94/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2324 - val_loss: 6968.2085\n",
            "Epoch 95/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2324 - val_loss: 6968.2690\n",
            "Epoch 96/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2280 - val_loss: 6968.2881\n",
            "Epoch 97/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2231 - val_loss: 6968.2891\n",
            "Epoch 98/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2192 - val_loss: 6968.2544\n",
            "Epoch 99/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2153 - val_loss: 6968.2798\n",
            "Epoch 100/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2144 - val_loss: 6968.3467\n",
            "Epoch 101/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2134 - val_loss: 6968.2842\n",
            "Epoch 102/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2124 - val_loss: 6968.3281\n",
            "Epoch 103/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2114 - val_loss: 6968.3765\n",
            "Epoch 104/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2109 - val_loss: 6968.2798\n",
            "Epoch 105/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2129 - val_loss: 6968.2681\n",
            "Epoch 106/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2109 - val_loss: 6968.3105\n",
            "Epoch 107/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2090 - val_loss: 6968.3203\n",
            "Epoch 108/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2085 - val_loss: 6968.4224\n",
            "Epoch 109/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2227 - val_loss: 6968.2422\n",
            "Epoch 110/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2183 - val_loss: 6968.2720\n",
            "Epoch 111/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2124 - val_loss: 6968.3125\n",
            "Epoch 112/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2104 - val_loss: 6968.2490\n",
            "Epoch 113/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2080 - val_loss: 6968.3452\n",
            "Epoch 114/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2061 - val_loss: 6968.4248\n",
            "Epoch 115/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2051 - val_loss: 6968.4238\n",
            "Epoch 116/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2139 - val_loss: 6968.2930\n",
            "Epoch 117/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2178 - val_loss: 6968.3862\n",
            "Epoch 118/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2129 - val_loss: 6968.3838\n",
            "Epoch 119/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2095 - val_loss: 6968.4170\n",
            "Epoch 120/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2075 - val_loss: 6968.4048\n",
            "Epoch 121/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2070 - val_loss: 6968.2891\n",
            "Epoch 122/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2090 - val_loss: 6968.3018\n",
            "Epoch 123/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2104 - val_loss: 6968.4351\n",
            "Epoch 124/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2090 - val_loss: 6968.2847\n",
            "Epoch 125/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2090 - val_loss: 6968.3794\n",
            "Epoch 126/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2090 - val_loss: 6968.3442\n",
            "Epoch 127/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2095 - val_loss: 6968.2861\n",
            "Epoch 128/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2070 - val_loss: 6968.3530\n",
            "Epoch 129/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2119 - val_loss: 6968.2954\n",
            "Epoch 130/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2305 - val_loss: 6968.2217\n",
            "Epoch 131/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2266 - val_loss: 6968.2212\n",
            "Epoch 132/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2192 - val_loss: 6968.2534\n",
            "Epoch 133/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2124 - val_loss: 6968.2383\n",
            "Epoch 134/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2173 - val_loss: 6968.2192\n",
            "Epoch 135/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2178 - val_loss: 6968.2207\n",
            "Epoch 136/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2114 - val_loss: 6968.2729\n",
            "Epoch 137/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2129 - val_loss: 6968.3394\n",
            "Epoch 138/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2095 - val_loss: 6968.3979\n",
            "Epoch 139/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2080 - val_loss: 6968.3418\n",
            "Epoch 140/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2070 - val_loss: 6968.3330\n",
            "Epoch 141/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2080 - val_loss: 6968.3428\n",
            "Epoch 142/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2085 - val_loss: 6968.2241\n",
            "Epoch 143/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2100 - val_loss: 6968.2808\n",
            "Epoch 144/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2144 - val_loss: 6968.2485\n",
            "Epoch 145/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2104 - val_loss: 6968.3096\n",
            "Epoch 146/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2100 - val_loss: 6968.2881\n",
            "Epoch 147/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2090 - val_loss: 6968.2510\n",
            "Epoch 148/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2056 - val_loss: 6968.2480\n",
            "Epoch 149/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2031 - val_loss: 6968.3018\n",
            "Epoch 150/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2026 - val_loss: 6968.2988\n",
            "Epoch 151/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2119 - val_loss: 6968.3091\n",
            "Epoch 152/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2051 - val_loss: 6968.2891\n",
            "Epoch 153/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2026 - val_loss: 6968.3252\n",
            "Epoch 154/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2007 - val_loss: 6968.3931\n",
            "Epoch 155/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1997 - val_loss: 6968.4019\n",
            "Epoch 156/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2002 - val_loss: 6968.3750\n",
            "Epoch 157/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2007 - val_loss: 6968.4492\n",
            "Epoch 158/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1992 - val_loss: 6968.5059\n",
            "Epoch 159/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1982 - val_loss: 6968.4443\n",
            "Epoch 160/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1987 - val_loss: 6968.5171\n",
            "Epoch 161/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1963 - val_loss: 6968.5742\n",
            "Epoch 162/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1958 - val_loss: 6968.5010\n",
            "Epoch 163/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1948 - val_loss: 6968.4644\n",
            "Epoch 164/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1958 - val_loss: 6968.5288\n",
            "Epoch 165/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1958 - val_loss: 6968.4463\n",
            "Epoch 166/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1958 - val_loss: 6968.4014\n",
            "Epoch 167/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2031 - val_loss: 6968.4175\n",
            "Epoch 168/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1982 - val_loss: 6968.5464\n",
            "Epoch 169/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1963 - val_loss: 6968.5264\n",
            "Epoch 170/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1982 - val_loss: 6968.5088\n",
            "Epoch 171/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2153 - val_loss: 6968.2583\n",
            "Epoch 172/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2334 - val_loss: 6968.2324\n",
            "Epoch 173/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2202 - val_loss: 6968.2432\n",
            "Epoch 174/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2197 - val_loss: 6968.2002\n",
            "Epoch 175/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2236 - val_loss: 6968.2061\n",
            "Epoch 176/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2256 - val_loss: 6968.2407\n",
            "Epoch 177/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2134 - val_loss: 6968.2402\n",
            "Epoch 178/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2080 - val_loss: 6968.2295\n",
            "Epoch 179/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2070 - val_loss: 6968.2622\n",
            "Epoch 180/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2031 - val_loss: 6968.2900\n",
            "Epoch 181/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2012 - val_loss: 6968.3330\n",
            "Epoch 182/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2041 - val_loss: 6968.2319\n",
            "Epoch 183/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2080 - val_loss: 6968.2881\n",
            "Epoch 184/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2046 - val_loss: 6968.3413\n",
            "Epoch 185/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2041 - val_loss: 6968.3218\n",
            "Epoch 186/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2051 - val_loss: 6968.3423\n",
            "Epoch 187/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2046 - val_loss: 6968.3555\n",
            "Epoch 188/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2119 - val_loss: 6968.3262\n",
            "Epoch 189/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2104 - val_loss: 6968.2544\n",
            "Epoch 190/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2207 - val_loss: 6968.2178\n",
            "Epoch 191/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2144 - val_loss: 6968.2417\n",
            "Epoch 192/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2095 - val_loss: 6968.3052\n",
            "Epoch 193/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2046 - val_loss: 6968.2920\n",
            "Epoch 194/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2007 - val_loss: 6968.4263\n",
            "Epoch 195/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2051 - val_loss: 6968.2666\n",
            "Epoch 196/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2046 - val_loss: 6968.2368\n",
            "Epoch 197/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2031 - val_loss: 6968.2979\n",
            "Epoch 198/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2007 - val_loss: 6968.3062\n",
            "Epoch 199/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1987 - val_loss: 6968.3242\n",
            "Epoch 200/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1973 - val_loss: 6968.2642\n",
            "Epoch 201/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1992 - val_loss: 6968.3574\n",
            "Epoch 202/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1973 - val_loss: 6968.3608\n",
            "Epoch 203/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1948 - val_loss: 6968.3169\n",
            "Epoch 204/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1958 - val_loss: 6968.3452\n",
            "Epoch 205/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1963 - val_loss: 6968.4341\n",
            "Epoch 206/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1938 - val_loss: 6968.5078\n",
            "Epoch 207/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1934 - val_loss: 6968.5225\n",
            "Epoch 208/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1938 - val_loss: 6968.4106\n",
            "Epoch 209/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1938 - val_loss: 6968.4775\n",
            "Epoch 210/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1958 - val_loss: 6968.4429\n",
            "Epoch 211/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1992 - val_loss: 6968.3276\n",
            "Epoch 212/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2070 - val_loss: 6968.2681\n",
            "Epoch 213/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2158 - val_loss: 6968.3521\n",
            "Epoch 214/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2139 - val_loss: 6968.2876\n",
            "Epoch 215/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2178 - val_loss: 6968.2568\n",
            "Epoch 216/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2148 - val_loss: 6968.2485\n",
            "Epoch 217/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2085 - val_loss: 6968.2705\n",
            "Epoch 218/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2026 - val_loss: 6968.2959\n",
            "Epoch 219/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2012 - val_loss: 6968.2773\n",
            "Epoch 220/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2002 - val_loss: 6968.3682\n",
            "Epoch 221/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1982 - val_loss: 6968.3525\n",
            "Epoch 222/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1958 - val_loss: 6968.2773\n",
            "Epoch 223/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1982 - val_loss: 6968.2310\n",
            "Epoch 224/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2002 - val_loss: 6968.2686\n",
            "Epoch 225/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1997 - val_loss: 6968.3032\n",
            "Epoch 226/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1992 - val_loss: 6968.3579\n",
            "Epoch 227/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1968 - val_loss: 6968.3564\n",
            "Epoch 228/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1958 - val_loss: 6968.3789\n",
            "Epoch 229/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1953 - val_loss: 6968.4419\n",
            "Epoch 230/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1953 - val_loss: 6968.3701\n",
            "Epoch 231/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1948 - val_loss: 6968.3711\n",
            "Epoch 232/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1943 - val_loss: 6968.4980\n",
            "Epoch 233/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1938 - val_loss: 6968.4683\n",
            "Epoch 234/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1938 - val_loss: 6968.4590\n",
            "Epoch 235/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1934 - val_loss: 6968.3472\n",
            "Epoch 236/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1938 - val_loss: 6968.2993\n",
            "Epoch 237/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1943 - val_loss: 6968.3335\n",
            "Epoch 238/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1973 - val_loss: 6968.3901\n",
            "Epoch 239/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1968 - val_loss: 6968.3340\n",
            "Epoch 240/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1982 - val_loss: 6968.3042\n",
            "Epoch 241/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2007 - val_loss: 6968.2700\n",
            "Epoch 242/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2017 - val_loss: 6968.2461\n",
            "Epoch 243/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2021 - val_loss: 6968.2871\n",
            "Epoch 244/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2056 - val_loss: 6968.3594\n",
            "Epoch 245/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2012 - val_loss: 6968.3428\n",
            "Epoch 246/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1992 - val_loss: 6968.3726\n",
            "Epoch 247/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2007 - val_loss: 6968.2969\n",
            "Epoch 248/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2002 - val_loss: 6968.3184\n",
            "Epoch 249/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1968 - val_loss: 6968.3149\n",
            "Epoch 250/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1968 - val_loss: 6968.4014\n",
            "Epoch 251/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1953 - val_loss: 6968.5010\n",
            "Epoch 252/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1938 - val_loss: 6968.3140\n",
            "Epoch 253/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1948 - val_loss: 6968.3926\n",
            "Epoch 254/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1943 - val_loss: 6968.3711\n",
            "Epoch 255/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1943 - val_loss: 6968.4292\n",
            "Epoch 256/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1953 - val_loss: 6968.4126\n",
            "Epoch 257/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1963 - val_loss: 6968.3984\n",
            "Epoch 258/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1929 - val_loss: 6968.4214\n",
            "Epoch 259/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1919 - val_loss: 6968.3218\n",
            "Epoch 260/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1914 - val_loss: 6968.3252\n",
            "Epoch 261/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1924 - val_loss: 6968.2529\n",
            "Epoch 262/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1924 - val_loss: 6968.3550\n",
            "Epoch 263/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1899 - val_loss: 6968.4878\n",
            "Epoch 264/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1919 - val_loss: 6968.4619\n",
            "Epoch 265/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1968 - val_loss: 6968.3389\n",
            "Epoch 266/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1982 - val_loss: 6968.3330\n",
            "Epoch 267/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1958 - val_loss: 6968.4302\n",
            "Epoch 268/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1953 - val_loss: 6968.3530\n",
            "Epoch 269/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1948 - val_loss: 6968.3535\n",
            "Epoch 270/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1929 - val_loss: 6968.3979\n",
            "Epoch 271/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1909 - val_loss: 6968.3281\n",
            "Epoch 272/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1948 - val_loss: 6968.3340\n",
            "Epoch 273/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2061 - val_loss: 6968.2593\n",
            "Epoch 274/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2090 - val_loss: 6968.4360\n",
            "Epoch 275/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2104 - val_loss: 6968.4175\n",
            "Epoch 276/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2080 - val_loss: 6968.3701\n",
            "Epoch 277/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2031 - val_loss: 6968.3589\n",
            "Epoch 278/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2031 - val_loss: 6968.3042\n",
            "Epoch 279/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2319 - val_loss: 6968.2290\n",
            "Epoch 280/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2236 - val_loss: 6968.2510\n",
            "Epoch 281/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2124 - val_loss: 6968.2202\n",
            "Epoch 282/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2090 - val_loss: 6968.2656\n",
            "Epoch 283/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2158 - val_loss: 6968.2749\n",
            "Epoch 284/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2104 - val_loss: 6968.2490\n",
            "Epoch 285/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2046 - val_loss: 6968.4155\n",
            "Epoch 286/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2026 - val_loss: 6968.3267\n",
            "Epoch 287/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2021 - val_loss: 6968.2314\n",
            "Epoch 288/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2217 - val_loss: 6968.1953\n",
            "Epoch 289/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2163 - val_loss: 6968.2021\n",
            "Epoch 290/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2080 - val_loss: 6968.2036\n",
            "Epoch 291/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2021 - val_loss: 6968.1914\n",
            "Epoch 292/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2002 - val_loss: 6968.2202\n",
            "Epoch 293/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1982 - val_loss: 6968.2671\n",
            "Epoch 294/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2002 - val_loss: 6968.2544\n",
            "Epoch 295/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1968 - val_loss: 6968.2305\n",
            "Epoch 296/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1973 - val_loss: 6968.2651\n",
            "Epoch 297/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1987 - val_loss: 6968.2256\n",
            "Epoch 298/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.2021 - val_loss: 6968.2700\n",
            "Epoch 299/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2002 - val_loss: 6968.2402\n",
            "Epoch 300/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1987 - val_loss: 6968.2979\n",
            "Epoch 301/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1982 - val_loss: 6968.3613\n",
            "Epoch 302/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1958 - val_loss: 6968.3882\n",
            "Epoch 303/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1929 - val_loss: 6968.3721\n",
            "Epoch 304/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1899 - val_loss: 6968.4492\n",
            "Epoch 305/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1890 - val_loss: 6968.4521\n",
            "Epoch 306/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1904 - val_loss: 6968.4404\n",
            "Epoch 307/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.4448\n",
            "Epoch 308/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1875 - val_loss: 6968.4878\n",
            "Epoch 309/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.4287\n",
            "Epoch 310/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1865 - val_loss: 6968.4492\n",
            "Epoch 311/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1880 - val_loss: 6968.5654\n",
            "Epoch 312/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.5674\n",
            "Epoch 313/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1865 - val_loss: 6968.6074\n",
            "Epoch 314/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1865 - val_loss: 6968.4541\n",
            "Epoch 315/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1865 - val_loss: 6968.4692\n",
            "Epoch 316/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1909 - val_loss: 6968.4658\n",
            "Epoch 317/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1895 - val_loss: 6968.3540\n",
            "Epoch 318/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1899 - val_loss: 6968.5825\n",
            "Epoch 319/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1904 - val_loss: 6968.3706\n",
            "Epoch 320/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1958 - val_loss: 6968.4492\n",
            "Epoch 321/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1929 - val_loss: 6968.3018\n",
            "Epoch 322/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1938 - val_loss: 6968.3809\n",
            "Epoch 323/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1953 - val_loss: 6968.3115\n",
            "Epoch 324/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1948 - val_loss: 6968.3579\n",
            "Epoch 325/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1963 - val_loss: 6968.3701\n",
            "Epoch 326/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2021 - val_loss: 6968.2891\n",
            "Epoch 327/512\n",
            "16/16 [==============================] - 24s 1s/step - loss: 7192.2012 - val_loss: 6968.4492\n",
            "Epoch 328/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1992 - val_loss: 6968.3486\n",
            "Epoch 329/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2007 - val_loss: 6968.3184\n",
            "Epoch 330/512\n",
            "16/16 [==============================] - 24s 1s/step - loss: 7192.1948 - val_loss: 6968.4014\n",
            "Epoch 331/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1919 - val_loss: 6968.3940\n",
            "Epoch 332/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1929 - val_loss: 6968.3643\n",
            "Epoch 333/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1987 - val_loss: 6968.2637\n",
            "Epoch 334/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1997 - val_loss: 6968.2837\n",
            "Epoch 335/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1963 - val_loss: 6968.3921\n",
            "Epoch 336/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1948 - val_loss: 6968.3979\n",
            "Epoch 337/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1919 - val_loss: 6968.3452\n",
            "Epoch 338/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1909 - val_loss: 6968.3530\n",
            "Epoch 339/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1899 - val_loss: 6968.3462\n",
            "Epoch 340/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1904 - val_loss: 6968.4146\n",
            "Epoch 341/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1914 - val_loss: 6968.3657\n",
            "Epoch 342/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1899 - val_loss: 6968.2881\n",
            "Epoch 343/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1885 - val_loss: 6968.3921\n",
            "Epoch 344/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.3442\n",
            "Epoch 345/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.2871\n",
            "Epoch 346/512\n",
            "16/16 [==============================] - 24s 1s/step - loss: 7192.1919 - val_loss: 6968.4458\n",
            "Epoch 347/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1899 - val_loss: 6968.2544\n",
            "Epoch 348/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1953 - val_loss: 6968.3428\n",
            "Epoch 349/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1982 - val_loss: 6968.4043\n",
            "Epoch 350/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1924 - val_loss: 6968.4209\n",
            "Epoch 351/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1919 - val_loss: 6968.4883\n",
            "Epoch 352/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1899 - val_loss: 6968.4072\n",
            "Epoch 353/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1914 - val_loss: 6968.2798\n",
            "Epoch 354/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1953 - val_loss: 6968.2573\n",
            "Epoch 355/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1943 - val_loss: 6968.3750\n",
            "Epoch 356/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1963 - val_loss: 6968.3389\n",
            "Epoch 357/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1919 - val_loss: 6968.3408\n",
            "Epoch 358/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1899 - val_loss: 6968.4268\n",
            "Epoch 359/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1899 - val_loss: 6968.3789\n",
            "Epoch 360/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1885 - val_loss: 6968.3779\n",
            "Epoch 361/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1904 - val_loss: 6968.3535\n",
            "Epoch 362/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1919 - val_loss: 6968.3931\n",
            "Epoch 363/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1899 - val_loss: 6968.3086\n",
            "Epoch 364/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1890 - val_loss: 6968.2803\n",
            "Epoch 365/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1899 - val_loss: 6968.4390\n",
            "Epoch 366/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1890 - val_loss: 6968.3916\n",
            "Epoch 367/512\n",
            "16/16 [==============================] - 28s 2s/step - loss: 7192.1924 - val_loss: 6968.5088\n",
            "Epoch 368/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1899 - val_loss: 6968.5518\n",
            "Epoch 369/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1934 - val_loss: 6968.3447\n",
            "Epoch 370/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2026 - val_loss: 6968.3252\n",
            "Epoch 371/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2065 - val_loss: 6968.2412\n",
            "Epoch 372/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1987 - val_loss: 6968.3770\n",
            "Epoch 373/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1968 - val_loss: 6968.3687\n",
            "Epoch 374/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1992 - val_loss: 6968.2500\n",
            "Epoch 375/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1943 - val_loss: 6968.2837\n",
            "Epoch 376/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1899 - val_loss: 6968.3569\n",
            "Epoch 377/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.3774\n",
            "Epoch 378/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1870 - val_loss: 6968.3955\n",
            "Epoch 379/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.3740\n",
            "Epoch 380/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1875 - val_loss: 6968.3794\n",
            "Epoch 381/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1865 - val_loss: 6968.4712\n",
            "Epoch 382/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1875 - val_loss: 6968.4976\n",
            "Epoch 383/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1870 - val_loss: 6968.6548\n",
            "Epoch 384/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1860 - val_loss: 6968.4922\n",
            "Epoch 385/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1870 - val_loss: 6968.4844\n",
            "Epoch 386/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.4282\n",
            "Epoch 387/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.3735\n",
            "Epoch 388/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.4810\n",
            "Epoch 389/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1885 - val_loss: 6968.4541\n",
            "Epoch 390/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1870 - val_loss: 6968.4419\n",
            "Epoch 391/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1851 - val_loss: 6968.4712\n",
            "Epoch 392/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1841 - val_loss: 6968.5830\n",
            "Epoch 393/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1885 - val_loss: 6968.2739\n",
            "Epoch 394/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1904 - val_loss: 6968.3340\n",
            "Epoch 395/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1909 - val_loss: 6968.3096\n",
            "Epoch 396/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1919 - val_loss: 6968.3809\n",
            "Epoch 397/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1885 - val_loss: 6968.4761\n",
            "Epoch 398/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1885 - val_loss: 6968.3765\n",
            "Epoch 399/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1904 - val_loss: 6968.2539\n",
            "Epoch 400/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2017 - val_loss: 6968.2222\n",
            "Epoch 401/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2041 - val_loss: 6968.2725\n",
            "Epoch 402/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1997 - val_loss: 6968.3115\n",
            "Epoch 403/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.2173 - val_loss: 6968.2329\n",
            "Epoch 404/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2300 - val_loss: 6968.1929\n",
            "Epoch 405/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2202 - val_loss: 6968.2661\n",
            "Epoch 406/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2100 - val_loss: 6968.2598\n",
            "Epoch 407/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2065 - val_loss: 6968.2617\n",
            "Epoch 408/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2100 - val_loss: 6968.2178\n",
            "Epoch 409/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.2061 - val_loss: 6968.2310\n",
            "Epoch 410/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2002 - val_loss: 6968.2969\n",
            "Epoch 411/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1992 - val_loss: 6968.2988\n",
            "Epoch 412/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1982 - val_loss: 6968.4141\n",
            "Epoch 413/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1958 - val_loss: 6968.3008\n",
            "Epoch 414/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1929 - val_loss: 6968.3501\n",
            "Epoch 415/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1904 - val_loss: 6968.3428\n",
            "Epoch 416/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1890 - val_loss: 6968.3999\n",
            "Epoch 417/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.3887\n",
            "Epoch 418/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1870 - val_loss: 6968.4219\n",
            "Epoch 419/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1865 - val_loss: 6968.3921\n",
            "Epoch 420/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.3018\n",
            "Epoch 421/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1885 - val_loss: 6968.4521\n",
            "Epoch 422/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1860 - val_loss: 6968.4731\n",
            "Epoch 423/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1865 - val_loss: 6968.3696\n",
            "Epoch 424/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1885 - val_loss: 6968.3101\n",
            "Epoch 425/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1885 - val_loss: 6968.3203\n",
            "Epoch 426/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.3364\n",
            "Epoch 427/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1890 - val_loss: 6968.3008\n",
            "Epoch 428/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1938 - val_loss: 6968.3389\n",
            "Epoch 429/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1914 - val_loss: 6968.4434\n",
            "Epoch 430/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1895 - val_loss: 6968.3887\n",
            "Epoch 431/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1860 - val_loss: 6968.3579\n",
            "Epoch 432/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1865 - val_loss: 6968.4175\n",
            "Epoch 433/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.4331\n",
            "Epoch 434/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1860 - val_loss: 6968.3936\n",
            "Epoch 435/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1851 - val_loss: 6968.5259\n",
            "Epoch 436/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1851 - val_loss: 6968.4785\n",
            "Epoch 437/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1836 - val_loss: 6968.5054\n",
            "Epoch 438/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1851 - val_loss: 6968.4932\n",
            "Epoch 439/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1851 - val_loss: 6968.4541\n",
            "Epoch 440/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1851 - val_loss: 6968.4282\n",
            "Epoch 441/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1836 - val_loss: 6968.4624\n",
            "Epoch 442/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1860 - val_loss: 6968.3633\n",
            "Epoch 443/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1865 - val_loss: 6968.4116\n",
            "Epoch 444/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1860 - val_loss: 6968.5093\n",
            "Epoch 445/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1851 - val_loss: 6968.5332\n",
            "Epoch 446/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1855 - val_loss: 6968.5532\n",
            "Epoch 447/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1851 - val_loss: 6968.4434\n",
            "Epoch 448/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1875 - val_loss: 6968.4321\n",
            "Epoch 449/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1865 - val_loss: 6968.4170\n",
            "Epoch 450/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1860 - val_loss: 6968.3477\n",
            "Epoch 451/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1890 - val_loss: 6968.3926\n",
            "Epoch 452/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1909 - val_loss: 6968.4595\n",
            "Epoch 453/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1895 - val_loss: 6968.4805\n",
            "Epoch 454/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1870 - val_loss: 6968.4546\n",
            "Epoch 455/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1841 - val_loss: 6968.4204\n",
            "Epoch 456/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1860 - val_loss: 6968.2939\n",
            "Epoch 457/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2021 - val_loss: 6968.2588\n",
            "Epoch 458/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.2007 - val_loss: 6968.2778\n",
            "Epoch 459/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1997 - val_loss: 6968.2852\n",
            "Epoch 460/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1958 - val_loss: 6968.3545\n",
            "Epoch 461/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1909 - val_loss: 6968.3965\n",
            "Epoch 462/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1948 - val_loss: 6968.3418\n",
            "Epoch 463/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1904 - val_loss: 6968.2817\n",
            "Epoch 464/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1919 - val_loss: 6968.2998\n",
            "Epoch 465/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1914 - val_loss: 6968.4443\n",
            "Epoch 466/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1904 - val_loss: 6968.4409\n",
            "Epoch 467/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1885 - val_loss: 6968.4526\n",
            "Epoch 468/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1870 - val_loss: 6968.4487\n",
            "Epoch 469/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1846 - val_loss: 6968.4087\n",
            "Epoch 470/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1836 - val_loss: 6968.5029\n",
            "Epoch 471/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1831 - val_loss: 6968.4429\n",
            "Epoch 472/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1870 - val_loss: 6968.3892\n",
            "Epoch 473/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1851 - val_loss: 6968.4023\n",
            "Epoch 474/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1860 - val_loss: 6968.4102\n",
            "Epoch 475/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.2993\n",
            "Epoch 476/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.3574\n",
            "Epoch 477/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1865 - val_loss: 6968.3643\n",
            "Epoch 478/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1855 - val_loss: 6968.3740\n",
            "Epoch 479/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1860 - val_loss: 6968.2891\n",
            "Epoch 480/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1895 - val_loss: 6968.3013\n",
            "Epoch 481/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1885 - val_loss: 6968.3403\n",
            "Epoch 482/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1870 - val_loss: 6968.3174\n",
            "Epoch 483/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1870 - val_loss: 6968.3750\n",
            "Epoch 484/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1865 - val_loss: 6968.5381\n",
            "Epoch 485/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1860 - val_loss: 6968.4111\n",
            "Epoch 486/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1870 - val_loss: 6968.4038\n",
            "Epoch 487/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1890 - val_loss: 6968.3135\n",
            "Epoch 488/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1948 - val_loss: 6968.4111\n",
            "Epoch 489/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1904 - val_loss: 6968.4990\n",
            "Epoch 490/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1890 - val_loss: 6968.3281\n",
            "Epoch 491/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1880 - val_loss: 6968.3862\n",
            "Epoch 492/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.4053\n",
            "Epoch 493/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1870 - val_loss: 6968.2939\n",
            "Epoch 494/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1880 - val_loss: 6968.2598\n",
            "Epoch 495/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1909 - val_loss: 6968.3379\n",
            "Epoch 496/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.5078\n",
            "Epoch 497/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1880 - val_loss: 6968.4868\n",
            "Epoch 498/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1865 - val_loss: 6968.5391\n",
            "Epoch 499/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1855 - val_loss: 6968.5171\n",
            "Epoch 500/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1855 - val_loss: 6968.5356\n",
            "Epoch 501/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1836 - val_loss: 6968.3955\n",
            "Epoch 502/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1836 - val_loss: 6968.4570\n",
            "Epoch 503/512\n",
            "16/16 [==============================] - 27s 2s/step - loss: 7192.1860 - val_loss: 6968.4043\n",
            "Epoch 504/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1865 - val_loss: 6968.2988\n",
            "Epoch 505/512\n",
            "16/16 [==============================] - 25s 2s/step - loss: 7192.1870 - val_loss: 6968.3149\n",
            "Epoch 506/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1875 - val_loss: 6968.5190\n",
            "Epoch 507/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1855 - val_loss: 6968.5010\n",
            "Epoch 508/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1841 - val_loss: 6968.5332\n",
            "Epoch 509/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1831 - val_loss: 6968.4331\n",
            "Epoch 510/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1826 - val_loss: 6968.4790\n",
            "Epoch 511/512\n",
            "16/16 [==============================] - 24s 2s/step - loss: 7192.1831 - val_loss: 6968.4551\n",
            "Epoch 512/512\n",
            "16/16 [==============================] - 26s 2s/step - loss: 7192.1826 - val_loss: 6968.5088\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 28, 28, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 28, 28, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 14, 14, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 14, 14, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTr  (None, 14, 14, 64)        16448     \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 14, 14, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 14, 14, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 14, 14, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 28, 28, 32)        8224      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 28, 28, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 28, 28, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 28, 28, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 28, 28, 1)         289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 184225 (719.63 KB)\n",
            "Trainable params: 183265 (715.88 KB)\n",
            "Non-trainable params: 960 (3.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predMnist = modelAutoMnist.predict(xTestMnist_new[:1200]) # просим модель вернуть нам изображение по первым 12тыс.\n",
        "\n",
        "predMnist = predMnist * 255 # представляем в виде значений от 0 до 255\n",
        "predMnist = predMnist.astype('uint8') # устанавливаем 8битовый тип\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73bHkxFXz1po",
        "outputId": "43c1a443-ae3d-40df-c446-cdf718295cea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 4s 93ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "BDVj4DeDjXVz"
      },
      "outputs": [],
      "source": [
        "def plotImages(xTestMnist_new, pred, shape=(28,28)):\n",
        "\n",
        "  n = 1  # количество картинок, которые хотим показать\n",
        "  plt.figure(figsize=(10, 4)) # указываем размеры фигуры\n",
        "\n",
        "  for i in range(n): # для каждой картинки из n\n",
        "    index = np.random.randint(0, pred.shape[0]) # startIndex - начиная с какого индекса хотим заплотить картинки\n",
        "    ax = plt.subplot(2, n, i + 1) # выведем область рисования Axes\n",
        "    plt.imshow(xTestMnist_new[index].reshape(shape))\n",
        "    plt.gray() # выведем в черно-белом цвете\n",
        "    ax.get_xaxis().set_visible(False) # скрываем вывод координатной оси x\n",
        "    ax.get_yaxis().set_visible(False) # скрываем вывод координатной оси y\n",
        "    ax = plt.subplot(2, n, i + 1 + n) # выведем область рисования Axes\n",
        "    plt.imshow(pred[index].reshape(shape)) # отрисуем обработанные сеткой картинки в размере 112*80\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotImages(xTestMnist_new, predMnist, shape=(28,28)) # покажем исходные и восстановленные картинки\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "1bm7h9Gbs2Mr",
        "outputId": "943635cc-918c-451e-9b3d-1900b7cc0bc7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_hPKMLv4PNO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}